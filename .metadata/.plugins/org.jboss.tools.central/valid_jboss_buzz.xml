<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Building Prometheus Dashboards in Business Central</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G7LBy7BAHUw/building-prometheus-dashboards-in-business-central.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2021/03/building-prometheus-dashboards-in-business-central.html</id><updated>2021-03-12T19:21:30Z</updated><content type="html">Dashbuilder is the project part of the Business Central suite responsible for dashboards and data sets. It can read data from multiple types of data set sources, including CSV, SQL, ElasticSearch, and Kie Server or you can create your own source of data using Java programming language. In jBPM 7.50.0 Final, we introduced a new type of provider for data sets: Prometheus. WHY PROMETHEUS Prometheus is the facto standard for collecting metrics. It has connectors to very well-known systems, such as Kafka and metrics can be easily consumed from third-party systems. Furthermore, Kie Server by default also exports ! Kie Server Prometheus Metrics PROMETHEUS IN BUSINESS CENTRAL To access Prometheus metrics from Business Central you must create a Data Set using Data Sets editor: Admin -&gt; Data Sets -&gt; New Data Set. Data Sets providers in Business Central Select Prometheus on the list and then provide the Prometheus installation in the form. The field “Query” accepts any , but pay attention to the fact that if a query does not result from data then no error is thrown, just an empty data set will be created Creating a Prometheus Data Set The result will contain at least the columns TIME and VALUE, then other columns will be created according to the result “metric” object. Sample Data Set data preview The above result was parsed from the following JSON: Prometheus Query Response SAMPLE DASHBOARD Let’s create some dashboards for the metrics exposed in Kie Server focusing on process instances metrics. The dashboard we will create can be filtered by container id and constantly pulls data from the server. Dashboard build using data coming from Prometheus To build our dashboard we need to do the following steps: 1. Create Prometheus data sets 2. Create the dashboard 3. Configure the dashboard for filtering and data refresh Let’s take a deep dive on those! STEP 1: CREATING PROMETHEUS DATA SETS We need to create 4 data sets. Notice that we recommend that you have some data available on Prometheus when creating the data sets, otherwise not all columns will be available. * Process Instances: Completed — The total number of processes completed. It uses the metrics kie_server_process_instance_completed_total * Process Instances: Completed Recently — a sliding time window that shows processes finished in the last 10 minutes, slicing by every 5 seconds. The query is: kie_server_process_instance_completed_total[10m:5s] * Process Instances: Completed — The total number of processes started. It uses the metric kie_server_process_instance_started_total * Process Instances: Started Recently — a sliding time window that shows processes started in the last 10 minutes, slicing by every 5 seconds. The query is: kie_server_process_instance_started_total[10m:5s] Now that we have data sets in place it is time to build the dashboard. STEP 2: BUILDING THE DASHBOARD Dashboards can be created in Business Central by selecting Pages from the main menu, check our about dashboards or the . Our simple dashboard contains a combo box to select containers, two bar charts, and two external components: Adding the container filter: In the page editor find Reporting group and drag a Filter to the page. Select combo box and use the data set Process Instances: Started using the container_id for the entries. Adding a Combo Box to filter the dashboard Adding bar charts: The data set for the bar chart uses the column value from Prometheus for the Y values and the process definition ID for the X-axis (categories). This step should be done for Process Instances: Started and Process Instances: Completed datasets. Creating Bar Charts to summarize the totals Adding time series: Time Series is a great created by my colleague . It has the power of transposing a data set column to be used as series. When transposing the data set, the time series can use the second data set column to build the series for the chart: Time Series component This step should be done for Process Instances: Started Recently and Process Instances: Completed Recently data sets. You may add static HTML elements to increase the dashboard, but from this point we are ready to start the configuration. Dashboard structure in Page Editor STEP 3: CONFIGURING FILTERING AND DATA REFRESH The final step to build our dashboard is to set up data refresh (polling) and filtering. Data Refresh: All the components have refresh turned on so it can poll data from Prometheus each X seconds. Select the component configuration by clicking on three dots in the upper right side of the component. Go to the Display tab and configure refresh on Refresh section. This must be done for all components that will be automatically refreshed. Data Refresh configuration Filtering: All components that will receive the filter must also have the Listen to Others flag on the Filter section selected. This way it will have the data set filtered when we select a container using the combo box we added earlier. Filtering configuration TESTING THE DASHBOARD Let’s test the dashboard by creating some data. We suggested using the Evaluation and Mortgage Process examples projects, but you can also create your own. This is our dashboard with data coming from Prometheus metrics: Dashboard in action with some data We can also filter the data to see the process from a specific container. Filtering by Container Now, your Dashbuilder dashboards based on Prometheus are ready for use! Now you can: * Add the dashboard to the Business Central so other uses can access it; * Export the Dashboard to run on . CONCLUSION In this post, we discussed the new Prometheus Data Set support in Business Central. In the next post let’s create a new dashboard for the Business Central tasks! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G7LBy7BAHUw" height="1" width="1" alt=""/&gt;</content><dc:creator>William Siqueira</dc:creator><feedburner:origLink>https://blog.kie.org/2021/03/building-prometheus-dashboards-in-business-central.html</feedburner:origLink></entry><entry><title>No more Java in vscode-xml 0.15.0!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/k1Sl-O2U6HY/" /><category term="Java" /><category term="Linux" /><category term="Mac" /><category term="VS Code" /><category term="Windows" /><category term="GraalVM" /><category term="vscode-xml" /><category term="XML extension for VS Code" /><author><name>David Thompson</name></author><id>https://developers.redhat.com/blog/?p=867757</id><updated>2021-03-12T08:00:43Z</updated><published>2021-03-12T08:00:43Z</published><content type="html">&lt;p&gt;Among other improvements and bug fixes in the &lt;code&gt;vscode-xml&lt;/code&gt; extension 0.15.0 release, you can now run the extension without needing &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. We know the Java requirement &lt;a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-xml&amp;#38;ssr=false#review-details"&gt;discouraged many people from trying the extension&lt;/a&gt;. We have included a new setting, &lt;b&gt;Prefer Binary&lt;/b&gt; (&lt;code&gt;xml.server.preferBinary&lt;/code&gt;) that lets you choose between the Java server and the new binary server. We&amp;#8217;re excited to remove the Java restriction from &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;Red Hat’s XML extension for Visual Studio Code&lt;/a&gt; in &lt;code&gt;vscode-xml&lt;/code&gt; 0.15.0. Keep reading to find out how we did it.&lt;/p&gt; &lt;h2&gt;LemMinX, Java, and vscode-xml&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx"&gt;Eclipse LemMinX&lt;/a&gt; is the language server that provides &lt;code&gt;vscode-xml&lt;/code&gt;&amp;#8216;s XML editing features. By creating a language server, we can provide XML editing capabilities not only to Visual Studio Code (&lt;a href="https://developers.redhat.com/blog/category/vs-code/"&gt;VS Code&lt;/a&gt;) but also to other text editors, such as Sublime, Eclipse IDE, Emacs, and Vim.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To learn more about language servers and the Language Server Protocol (LSP), please see &lt;a href="https://developers.redhat.com/blog/2016/06/27/a-common-interface-for-building-developer-tools/"&gt;&lt;i&gt;A common interface for building developer tools&lt;/i&gt;&lt;/a&gt;, which does a great job of explaining the topic.&lt;/p&gt; &lt;p&gt;LemMinX was written in Java. Using Java to code LemMinX allows for using existing XML libraries. For instance, LemMinX uses the Xerces library to validate XML files against schemas. Until the &lt;code&gt;vscode-xml&lt;/code&gt; 0.15.0 release, we required a Java Runtime Environment (JRE), because we distributed LemMinX as a JAR file that had to be interpreted by the Java runtime. LemMinX also supports a number of useful extensions, either as individual JAR files or as a ZIP file containing multiple JARs. Current extensions include &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx-maven"&gt;lemminx-maven &lt;/a&gt; (for Maven &lt;code&gt;pom.xml&lt;/code&gt; files), &lt;a target="_blank" rel="nofollow" href="https://github.com/Treehopper/liquibase-lsp"&gt;liquibase-lsp&lt;/a&gt; (for liquibase XML files), and &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/liberty-language-server/tree/master/lemminx-liberty"&gt;lemminx-liberty&lt;/a&gt; (for OpenLiberty &lt;code&gt;server.xml&lt;/code&gt; files).&lt;/p&gt; &lt;p&gt;To allow the user to run &lt;code&gt;vscode-xml&lt;/code&gt; without installing Java, we needed to rethink how we provided LemMinX functionality to the user.&lt;/p&gt; &lt;h2&gt;GraalVM native-image&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.graalvm.org/reference-manual/native-image/"&gt;GraalVM native-image&lt;/a&gt; is a tool that can compile a Java program into a standalone, platform-specific executable binary. Unlike other tools, it doesn’t only package a copy of Java along with the program. GraalVM compiles the Java bytecode into native instructions, and it includes a small library that manages memory. When we use GraalVM, the generated binary doesn&amp;#8217;t include all the code that normally goes into a Java installation. The executable binary in LemMinX is even smaller than &lt;a target="_blank" rel="nofollow" href="https://download.eclipse.org/justj/jres/11/downloads/latest/"&gt;these minimal Java installations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 shows the sequence by which GraalVM native-image creates an executable. The Java compiler transforms Java source code into Java bytecode. Then, GraalVM native-image turns the Java bytecode into native instructions.&lt;/p&gt; &lt;div id="attachment_867787" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png"&gt;&lt;img aria-describedby="caption-attachment-867787" class="wp-image-867787" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png" alt="The steps to transform Java source code into a native binary." width="640" height="88" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram.png 780w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram-300x41.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/complation-diagram-768x105.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-867787" class="wp-caption-text"&gt;Figure 1: How GraalVM native-image transforms Java bytecode into native instructions.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;GraalVM native-image has limitations. The binary that GraalVM produces works only for a specific operating system and CPU architecture. As a result, we need to compile and distribute separate binaries for Windows, macOS, and &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;. The macOS binary is compiled for the x86_64 CPU architecture but also works on Apple Silicon through the &lt;a target="_blank" rel="nofollow" href="https://developer.apple.com/documentation/apple_silicon/about_the_rosetta_translation_environment"&gt;Rosetta 2 translation layer&lt;/a&gt;. Another limitation is that all the code that you want to run must be present during the native-image process. &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/blob/master/docs/Extensions.md#custom-xml-extensions"&gt;Extensions to the base XML editing functionality&lt;/a&gt;, such as &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/lemminx-maven"&gt;lemminx-maven&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://github.com/Treehopper/liquibase-lsp"&gt;liquibase-lsp&lt;/a&gt;, and &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/liberty-language-server/tree/master/lemminx-liberty"&gt;lemminx-liberty&lt;/a&gt;, won&amp;#8217;t work when using the binary server.&lt;/p&gt; &lt;h2&gt;Delivering the binaries&lt;/h2&gt; &lt;p&gt;We do not package the binaries into &lt;code&gt;vscode-xml&lt;/code&gt;. Instead, when &lt;code&gt;vscode-xml&lt;/code&gt; starts up and detects that you don’t have Java installed, it downloads the binary specific to your computer’s operating system. This workaround reduces the total amount of data downloaded to make &lt;code&gt;vscode-xml&lt;/code&gt; work. The extension comes with the SHA 256 checksums of the binaries so that it can check the integrity of the downloaded binary.&lt;/p&gt; &lt;h2&gt;When will vscode-xml use a binary?&lt;/h2&gt; &lt;p&gt;You can use the flowchart in Figure 2 to figure out whether &lt;code&gt;vscode-xml&lt;/code&gt; will download and run a binary. Note that you can enable the &lt;b&gt;Prefer Binary&lt;/b&gt; setting (&lt;code&gt;xml.server.preferBinary&lt;/code&gt;) if you have a Java runtime but still want VS Code to use the binary.&lt;/p&gt; &lt;div id="attachment_867797" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png"&gt;&lt;img aria-describedby="caption-attachment-867797" class="wp-image-867797" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png" alt="The steps required in sequence to determine whether a binary or Java server should be started." width="640" height="328" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart.png 1003w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart-300x154.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/which-server-version-flowchart-768x394.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-867797" class="wp-caption-text"&gt;Figure 2: How vscode-xml determines whether to start a binary or a Java server.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If Java is not installed, &lt;code&gt;vscode-xml&lt;/code&gt; launches the binary server. If it is installed and the &lt;b&gt;Prefer Binary&lt;/b&gt; setting is &lt;em&gt;disabled&lt;/em&gt;, &lt;code&gt;vscode-xml&lt;/code&gt; launches the Java server. If it is installed and the &lt;b&gt;Prefer Binary&lt;/b&gt; setting is &lt;em&gt;enabled&lt;/em&gt;, &lt;code&gt;vscode-xml&lt;/code&gt; launches the Java server if LemMinX extensions are detected; otherwise, it launches the binary server.&lt;/p&gt; &lt;p&gt;The LemMinX server extensions shown in Figure 2 are extensions onto &lt;code&gt;vscode-xml&lt;/code&gt;’s functionality, as I described earlier. If the extensions are present along with the Java runtime, they override the &lt;b&gt;Prefer Binary&lt;/b&gt; setting because these extensions won’t work with the binary server.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article offered an inside look at how we removed the Java requirement in &lt;code&gt;vscode-xml&lt;/code&gt;. Thanks to everybody who contributed to this release! To see a list of all the changes and bug fixes, please see the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/blob/master/CHANGELOG.md"&gt;vscode-xml 0.15.0 changelog&lt;/a&gt;. If you encounter bugs or have ideas for improving &lt;code&gt;vscode-xml&lt;/code&gt;, we hope you will &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/vscode-xml/issues/new/choose"&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#38;linkname=No%20more%20Java%20in%20vscode-xml%200.15.0%21" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F12%2Fno-more-java-in-vscode-xml-0-15-0%2F&amp;#038;title=No%20more%20Java%20in%20vscode-xml%200.15.0%21" data-a2a-url="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/" data-a2a-title="No more Java in vscode-xml 0.15.0!"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/"&gt;No more Java in vscode-xml 0.15.0!&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/k1Sl-O2U6HY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Among other improvements and bug fixes in the vscode-xml extension 0.15.0 release, you can now run the extension without needing Java. We know the Java requirement discouraged many people from trying the extension. We have included a new setting, Prefer Binary (xml.server.preferBinary) that lets you choose between the Java server and the new binary server. [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/"&gt;No more Java in vscode-xml 0.15.0!&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">867757</post-id><dc:creator>David Thompson</dc:creator><dc:date>2021-03-12T08:00:43Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/</feedburner:origLink></entry><entry><title type="html">Configuring DNS ping on a WildFly Bootable JAR cluster application</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/T_qivNQqGgE/" /><author><name>Yeray Borges</name></author><id>https://wildfly.org//news/2021/03/12/Bootable-jar-jkube-clustering-dns-ping-openshift/</id><updated>2021-03-12T00:00:00Z</updated><content type="html">Note This post is a continuation of my blog post. It assumes you are familiar with the we used for that article. INTRODUCTION We recently described in this details about how to deploy and configure a WildFly Bootable JAR cluster application on OpenShift. By default, a Bootable JAR cluster application uses the protocol for its clustering discovery mechanism. This protocol is based on the ability to query the Kubernetes API to determine which pods match certain criteria and then create a JGroups cluster based on this information. This protocol may not be a viable or an ideal solution for your scenario; for example, if you cannot grant a specific permission on your Kubernetes cluster to use KUBE_PING, or if your requirement is to make your application agnostic to the Kubernetes API. In this guide we will explain how you can configure the same Bootable JAR cluster application but this time using the protocol. GETTING STARTED The following are the key points we need to accomplish to configure our Bootable JAR cluster application to use DNS_PING: * Define a ping service. We need to create a resource, which is used by the protocol to get the IP addresses of the pods that would be part of the cluster. When we are defining this resource, we need to pay attention to the service selector and service name. The selector defines the pods that will be backed by this service. The JGroups subsystem needs to know the service name to configure the DNS_PING protocol. * Configure the JGroups subsystem to use DNS_PING. We need to change the default JGroups subsystem configuration of our Bootable JAR application. We will execute a CLI script at build time to configure the JGroups protocol stack, removing the default KUBE_PING protocol and adding the DNS_PING. One important detail about the DNS_PING configuration is the value we will use for the dns_query property. This value specifies the DNS name of the Kubernetes ping service the protocol is going to use to discover other cluster members. To sum up, we will need a Kubernetes service with a matching label selector to define which pods will be part of our cluster, and we will tell the JGroups subsystem that we want to use such a service to locate other possible cluster members. DEFINE A PING SERVICE The ping service is just an additional Kubernetes service resource. We are using the to deploy our application. This maven plugin allows us to specify any additional resource we want to deploy. What we need to do is create our additional resources under src/main/jkube/ and JKube will create the resource for us when deploying the application. This is our ping service definition at : apiVersion: v1 kind: Service metadata: name: wildfly-demo-ping-service annotations: description: The JGroups ping service for clustering discovery. spec: clusterIP: None publishNotReadyAddresses: true selector: app: wildfly-clustering-demo The selector specifies the name of our application. Without any additional modification, the name of our application is the maven group id of our project. We have also configured the service with publishNotReadyAddresses: true. This configuration specifies that when there is a DNS lookup by using this service, the result will contain all pods matching the service’s label selector, even those which are not yet in a ready state. We want the JGroups cluster to form before the pods are receiving end user requests. This configuration makes it possible by returning also the IP of the pods that are not ready yet. We have also added clusterIP: None in the service spec. This means this service will not assign a cluster IP through which clients can connect to all the pods backing it; instead it will return an IP to connect directly to the pod. CONFIGURE THE JGROUPS SUBSYSTEM TO USE DNS_PING. As a second step, we need to configure the JGroups stack to use the DNS_PING protocol. We have to instruct the to execute the : /subsystem=jgroups/stack=tcp/protocol=kubernetes.KUBE_PING:remove /subsystem=jgroups/stack=tcp/protocol=dns.DNS_PING:add(add-index=0) /subsystem=jgroups/stack=tcp/protocol=dns.DNS_PING/property=dns_query:add(value=${env.DNS_PING_SERVICE_NAME}) /subsystem=jgroups/stack=tcp/protocol=dns.DNS_PING/property=async_discovery_use_separate_thread_per_request:add(value=true) The script removes the existing kubernetes.KUBE_PING protocol and adds the dns.DNS_PING. The value of the dns_query property will be retrieved from an environment variable named DNS_PING_SERVICE_NAME. The value of this environment variable is specified under the &lt;env&gt; section of the JKube maven plugin configuration in our pom.xml. The maven openshift profile looks as : &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;openshift&lt;/id&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-jar-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;cloud/&gt; &lt;cli-sessions&gt; &lt;cli-session&gt; &lt;script-files&gt; &lt;script&gt;${project.build.scriptSourceDirectory}/configure_dns_ping.cli&lt;/script&gt; &lt;/script-files&gt; &lt;/cli-session&gt; &lt;/cli-sessions&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jkube&lt;/groupId&gt; &lt;artifactId&gt;openshift-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.2&lt;/version&gt; &lt;configuration&gt; &lt;resources&gt; &lt;env&gt; ... &lt;DNS_PING_SERVICE_NAME&gt;wildfly-demo-ping-service&lt;/DNS_PING_SERVICE_NAME&gt; &lt;/env&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;resource&lt;/goal&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; BUILD, DEPLOY AND VERIFY THE DEMO APPLICATION Now let us build and deploy our Bootable JAR application. We assume you have a cluster running and you have already logged into it. 1. Clone the demo application and checkout the dns-ping branch. $ git clone https://github.com/yersan/wildfly-clustering-demo.git $ cd wildfly-clustering-demo wildfly-clustering-demo (master) $ git checkout dns-ping Switched to branch 'dns-ping' 2. Build and deploy the application by issuing the following maven goal: wildfly-clustering-demo (dns-ping) $ mvn oc:deploy -Popenshift 3. Once your application has been completely deployed, scale it up, for example, three replicas: wildfly-clustering-demo (dns-ping) $ oc scale dc/wildfly-clustering-demo --replicas=3 deploymentconfig.apps.openshift.io/wildfly-clustering-demo scaled 4. Once your pods are in ready state, you can inspect the logs of any pod and verify there are three members in the cluster: wildfly-clustering-demo (dns-ping) $ oc get pods NAME READY STATUS RESTARTS AGE wildfly-clustering-demo-3-9dmrk 1/1 Running 0 117s wildfly-clustering-demo-3-deploy 0/1 Completed 0 3m6s wildfly-clustering-demo-3-f99qb 1/1 Running 0 3m2s wildfly-clustering-demo-3-snh74 1/1 Running 0 117s wildfly-clustering-demo-s2i-1-build 0/1 Completed 0 5m31s wildfly-clustering-demo (dns-ping) $ oc logs -f pods/wildfly-clustering-demo-3-snh74 ... 11:05:33,906 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 50) ISPN000078: Starting JGroups channel ee 11:05:33,908 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 50) ISPN000094: Received new cluster view for channel ee: [clustering-demo-3-f99qb|2] (3) [clustering-demo-3-f99qb, clustering-demo-3-9dmrk, clustering-demo-3-snh74] 11:05:33,911 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 50) ISPN000079: Channel ee local address is clustering-demo-3-snh74, physical addresses are [10.129.148.40:7600] ... You should have at this point the Bootable JAR application running on a cluster of three pods. CONCLUSION You can easily configure a Bootable JAR application and adapt it to your needs by executing a CLI script. In this article, we have seen a practical example of how to configure the JGroups protocol stack. Together with the ability to deploy additional resources given by the JKube maven plugin, we have replaced the default clustering discovery mechanism by adding minimal changes to our project. You can find out more examples of how to use and work with the Bootable JAR . If you have any question related, feel free to contact us joining to the or .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/T_qivNQqGgE" height="1" width="1" alt=""/&gt;</content><dc:creator>Yeray Borges</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/12/Bootable-jar-jkube-clustering-dns-ping-openshift/</feedburner:origLink></entry><entry><title type="html">MicroProfile Reactive Messaging in WildFly 23, and WildFly MicroProfile Reactive Specifications Feature Pack 2.0.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZjGnSybcPN0/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2021/03/11/WildFly-MicroProfile-Reactive-specifications-feature-pack-2.0/</id><updated>2021-03-11T12:00:00Z</updated><content type="html">I am pleased to announce the 2.0.0.Final release of the MicroProfile Reactive Specifications Feature Pack for WildFly. Between the and the release, we pulled the core of what the feature pack contained into WildFLy. WildFly now contains these Galleon layers which used to live in the 1.0.x stream of the feature pack: * microprofile-reactive-messaging - Provides the functionality - this is a framework for building event-driven, data streaming and event sourcing applications using CDI. The streams, or channels, can be backed by a variety of messaging technologies. * microprofile-reactive-messaging-kafka - The include the connector for Kafka in WildFly, in the layer * microprofile-reactive-streams-operators - provides the functionality. The WildFly 23.0.0.Final zip available from our page contains these layers, however they are not enabled by default. To enable the functionality you need to add the extensions and enable the subsystems. The simplest way is to run this . This script is taken from the WildFly Reactive Messaging with Kafka QuickStart, which you can find to get you started. Additionally there are sections about these subsystems in our Admin Guide (/). THE FEATURE PACK As before to use the feature pack you will need to use Galleon to provision a server, as pointed out in the feature pack . That README contains more details of what is contained, but in summary it contains Galleon layers to provide the following functionality: * functionality. * Additional Reactive Messaging connectors for: * AMQP * MQTT We decided to remove RxJava2 support of context propagation for the feature pack since that caused some problems under the hood. RxJava2 is not a supported API for user applications in WildFly (although we use it for somem internal functionality). If you have the need to process streams, please use the MicroProfile Reactive Streams Operators API instead. Note that the 2.0.x series of the feature pack will only work with WildFLy 23. For earlier WildFly versions, use the 1.0.x releases. The present latest release on that stream, 1.0.2, works with WildFLy 21 and WildFly 22. SPEC COMPLIANCE It is worth pointing out that we’re strictly staying with what version 1.0 of the MicroProfile Reactive Messaging specification provides. However, the SmallRye Reactive Messaging we use is used to develop the next version of the specification, which is not ready yet. If you wish to get a preview of that, the steps are to instead of compiling your application against org.eclipse.microprofile.reactive.messaging:microprofile-reactive-messaging-api:1.0, compile against io.smallrye.reactive:smallrye-reactive-messaging-api:3.0.0, and make sure you start the server with -Djboss.as.reactive.messaging.experimental=true which will bypasss some checks and allow you to use more recent, although currently unreleased constructs such as and . Note: - these APIs may still change until there is a final release of the next specification version. WHAT IS COMING UP? Once the MicroProfile Reactive Messaging 2.0 specification is released and the other reactive specifications, MicroProfile Reactive Streams Operators and MicroProfile Context Propagation, are finalised, we will start work on integrating them into a future (as yet to be determined) WildFly version. With the current information, the feature pack then mainly becomes a place for the connectors we don’t want in WildFly yet. FEEDBACK We’re keen to hear your feedback! Please raise any issues found with the feature pack at . And for the parts in WildFly, raise issues at , in the WFLY project (using 'MP Reactive Messaging' as the component).&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZjGnSybcPN0" height="1" width="1" alt=""/&gt;</content><dc:creator>Kabir Khan</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/11/WildFly-MicroProfile-Reactive-specifications-feature-pack-2.0/</feedburner:origLink></entry><entry><title type="html">Supply chain integration - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0YLeOL6ZVQY/supply-chain-integration-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/2a7zREqrrec/supply-chain-integration-common-architectural-elements.html</id><updated>2021-03-11T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements  In  from this series I introduced a use case around supply chain integration for retail stores. The process was laid out how I've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the supply chain integration architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's my intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but I've chosen a format that I hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that I've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that I've identified and collected in to the generic architectural blueprint.  It's my intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's my job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but I've chosen some icons, text and colours that I hope are going to make it all easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. CONTAINER PLATFORM Without a doubt, every modern organisation engaged in business optimisation has seen the value of containers and use of a container platform. The container platform provides for one consistent environment for developers and operations to manage services, applications, integration points, process integration, planning services, and security. It's also the one way to ensure you can uniformly leverage the same container infrastructure across a hybrid multicloud environment. It avoids becoming locked into any private or cloud infrastructure as you have an exit strategy with a container platform that's consistent across your architecture. There are a few elements here worth mentioning, first off the use of supply chain microservices for centralising all interactions with supply chain relevant systems of record and provides access to other services. An api management element for well defined access to services and events, and both message transformation and event streaming services to react and transform communication messages across the platform. Finally, there are elements representing collections of integration microservices and integration data microservices for storage service access. The security aspect is interwoven in the container platform, as each container service, application, or integration can be plugged in to an organisations authentication and authorization mechanisms. INFRASTRUCTURE SERVICES These elements in the common architecture are found in the solutions researched. They were mentioned by name and consisted of an single-sign-on (SSO) that ensures a smooth interaction between processes, authorisation, authentication, and integration services. The AI / Machine Learning platform, shown with a private cloud icon, can be any modern data platform  that are managed and deployed in this organisation's infrastructure to support the retail usage of AI / ML by ensuring access to supply chain data. EXTERNAL SYSTEMS There one more element that represents the external supply chain systems that are integrated with the core elements of this architecture.  As there are often many third-party systems, this element covers basically everything that customers use from partnering ventors. This can be SaaS solutions or any other third-party backend systems. STORAGE SERVICES The storage services uncovered in the research were pretty simplistic and for that reason there's a single physical block storage element.  In later articles, when more detail is shown, I'll make a point to mention the link to a separate architecture blueprint supporting the retail data framework which is linked to this use case. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the supply chain integration use case.  An overview of this series on the supply chain integration portfolio architecture blueprint can be found here: 1. 2. 3. Example supply chain integration Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example supply chain integration architecture to provide you with a map for your own supply chain solutions. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0YLeOL6ZVQY" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/2a7zREqrrec/supply-chain-integration-common-architectural-elements.html</feedburner:origLink></entry><entry><title type="html">WildFly 23 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XiMb74ygcHs/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/03/11/WildFly23-Final-Released/</id><updated>2021-03-11T00:00:00Z</updated><content type="html">I’m pleased to announce that the WildFly 23 Final zip is now available . It’s been a busy time since the January WildFly 22 release, with a bit shorter development cycle than normal. But a lot has been accomplished. Let’s have a look at what’s new. NEW FEATURES ECLIPSE MICROPROFILE 4.0 In this release we have moved our Eclipse MicroProfile platform implementations from the 3.3 platform specification versions to the 4.0 versions. WildFly 23 supports the following Eclipse MicroProfile platform specifications: Specification Version in WildFly 23 MicroProfile Config MicroProfile Fault Tolerance MicroProfile Health MicroProfile JWT Authentication MicroProfile Metrics MicroProfile OpenAPI MicroProfile OpenTracing MicroProfile Rest Client We also provide all of the MicroProfile specs that are also part of Jakarta EE 8. Please note that an . This may result in incompatible changes between specification releases, which WildFly will necessarily reflect. With MicroProfile 4.0 the following specifications include API incompatible changes: * * * * * MICROPROFILE REACTIVE MESSAGING WildFly 23 now provides . This includes providing a connector for interaction with Kafka streams. This capability adds support for two more MicroProfile specifications to WildFly, implemented via two new extensions and subsystems: Specification Version in WildFly 23 MicroProfile Reactive Messaging MicroProfile Reactive Streams Operators Please note that these extensions and subsystems are not included by default in the standard standalone.xml files that WildFly provides. Users who want them can add them to their configuration by using Galleon to or and telling Galleon to include the new Galleon layers we’ve added for these. Or, use our zip and use the CLI to add the new extensions and subsystems. Further details can be . EXPRESSION RESOLUTION FROM A CREDENTIAL STORE WildFly 23 adds support for expressions in the management model to be . This enhancement makes use of a new resource expression-encryption in the elytron subsystem to configure the expression resolution. This new resource also contains a management operation create-expression which allows users to create encrypted expressions using the usual management clients. In addition to the new resource for expression resolution a new secret-key-credential-store has been added for the purpose of providing an initial secret key to the application server process. In the past users needed to rely on masking a password but this was achieved using a well known public password and password based encryption. Starting from a secret key allows administrators to manage their own initial secret. Both this new credential store resource and the existing credential-store resource have been updated to support the generation of secret keys as well as the ability to export and import previously generated secret keys. Finally the wildfly-elytron-tool has also been updated to support both types of credential store and the credential-store command updated to support management of secret keys and the generation of encrypted tokens for use in expressions. Users of the deprecated Picketbox-vault-backed expression resolution mechanism are strongly encouraged to move to this new feature, as our intent is to remove support for Picketbox and the Picketbox-vault in an upcoming release. PROVISIONING AND MANAGING WILDFLY * For users who wish to deploy multiple applications on the same server instance where one application during startup needs to make an external (i.e. over the network) invocation on another, we have provided a mechanism that prevents such requests being accepted. Not handling such requests can prevent startup of the deployment making them and prevent the server booting. A number of users have asked for this use case tp be supported, so we have done so, but this is not a recommended deployment architecture. A server that is not booting gracefully may receive incoming requests that it is not yet ready to handle, resulting in errors. * There is now . A common file can be used to set up the environment for all the shell scripts that WildFly provides. For example, you can set the JAVA_HOME in a common.conf script configuration file to ensure the same version of Java is used for all scripts. * When launching a WildFly bootable jar, users can as part of boot. This provides a mechanism for performing final configuration that cannot be accomplished via the preferred approach of configuring at build time or via typical runtime customization mechanisms like setting environment variables or system properties. This support is Tech Preview as the mechanism may change in later releases. MESSAGING * The management API can now be used to for investigative purposes. * The behavior of the embedded Artemis broker’s critical-analyzer feature . * The embedded Artemis broker can be configured to for subsequent investigation instead of immediately deleting it. * A call-timeout attribute has been . The attribute specifies the time out for blocking calls performed by a core bridge. OTHER AREAS * The Jakarta Concurrency managed executors provided by the ee subsystem can be that have been executing for an unexpectedly long time. Such tasks can also be manually terminated. * The transaction subsystem now supports for transactions. * The undertow subsystem can now be included as part of the request and response JSESSION_ID cookie. * Deployments can now depend on and use the APIs provided by the following Infinispan-related modules without getting a private API usage warning: * org.infinispan (embedded cache) * org.infinispan.client.hotrod (client for remote infinispan server) * org.infinispan.commons * Principal propagation of EJBs was different for legacy security and Elytron security in some cases. To provide a possibility to configure which behaviour should apply, we legacy-compliant-principal-propagation to application-security-domain component in the ejb3 subsystem. This attribute is optional and the principal propagation is legacy compliant by default. WILDFLY PREVIEW As I when we released WildFly 22 Alpha1, along with our traditional Jakarta EE 8 distribution we want to give our users a preview of what will be coming in WildFly as we move on to EE 9 and later. We call this distribution "WildFly Preview". The WildFly 23.0.0.Final release includes an update to WildFly Preview. Even though this is coming from a .Final tag of the WildFly codebase, WildFly Preview should always be regarded as a tech-preview/beta distribution. EE 9 is primarily about implementing the necessary change in the Jakarta EE APIs from the javax.* package namespace to the jakarta.* namespace. This is a big change that is going to take a while to percolate through the EE ecosystem, e.g. for the many projects that compile against the EE APIs to provide versions that use jakarta.*. While this happens we want to continue to deliver new features and fixes to our community, so the primary WildFly distribution will continue to provide the EE 8 APIs. FEATURE PACK CHANGES WildFly users can use Galleon feature packs to or . The WildFly project produces five different feature packs: wildfly-core, wildfly-servlet, wildfly-ee, wildfly and wildfly-preview. The composition of these feature packs has changed somewhat in WildFly 23, in that the wildfly-ee feature pack no longer depends on wildfly-servlet or (transitively) wildfly-core. Instead it directly incorporates the same content that was previously made available via a dependency relationship. For most users, this subtle difference should have no impact. However, there are some cases where it might: * If you are producing your own feature pack that depends on wildfly or wildfly-ee you may need to adjust your pom.xml and wildfly-feature-pack-build.xml to remove any dependency on wildfly-servlet and wildfly-core. * If your build uses another feature pack that depends on the wildfly or wildfly-ee feature packs, you should wait to upgrade to WildFly 23 until a release of that feature pack that depends on the WildFly 23 packs is available. A commonly used example of this is the feature pack. Users of that feature pack should move to the 1.2.3.Final release that came out today. The WildFly project still produces the wildfly-core and wildfly-servlet feature packs for those who wish to use them, although they may be discontinued at some point. STANDARDS SUPPORT WildFly 23.0.0 is a Jakarta EE 8 compatible implementation, with both the Full Platform and the Web Profile. Evidence supporting our certification is available and . Beginning with WildFly 23 we will be exclusively focusing on the Jakarta EE test suite for EE certification / compliance. WildFly 23 is also a compliant implementation of the Eclipse MicroProfile 4.0 platform specification. The WildFly Preview distribution released today is not yet a compatible implementation of Jakarta EE 9 or MicroProfile 4.0. We’re continuing to make good progress toward being able to certify compatibility, but we’re not there yet. The main area where users may hit meaningful issues related to EE compliance is in webservices if deployment descriptors using the EE 9 xml schemas are used. This can be worked around by using EE 8 schemas, which are functionally equivalent. JDK SUPPORT Our recommendation is that you run WildFly on the most recent long-term support JDK release, i.e. on JDK 11 for WildFly 23. While we do do some testing of WildFly on JDK 12 and 13, we do considerably more testing of WildFly itself on the LTS JDKs, and we make no attempt to ensure the projects producing the various libraries we integrate are testing their libraries on anything other than JDK 8 or 11. WildFly 23 also is heavily tested and runs well on Java 8. We plan to continue to support Java 8 at least through WildFly 24, and probably beyond. While we recommend using an LTS JDK release, I do believe WildFly runs well on JDK 13. By run well, I mean the main WildFly testsuite runs with no more than a few failures in areas not expected to be commonly used. We want developers who are trying to evaluate what a newer JVM means for their applications to be able to look to WildFly as a useful development platform. We do see a couple of test failures with JDK 13 when using the deprecated Picketlink subsystem and WS Trust. Work to allow WildFly to run on JDK 15 and later is ongoing. We’re continuing our work to digest fully some of the package removals that came in JDK 14, particularly in the security area. The biggest barrier we face is the deprecated legacy security implementation based on Picketbox cannot support JDK 14. We intend to remove support for that security implementation quite soon and to only provide Elytron-based security. Please note that WildFly runs on Java 11 and later in classpath mode. DOCUMENTATION The WildFly 23 documentation is available at the . The WildFly 23 management API documentation is in the . JIRA RELEASE NOTES The full list of issues resolved is available . Issues resolved in the WildFly Core 15 release included with WildFly 23 are available . ENJOY! Thank you for your continued support of WildFly. We’d love to hear your feedback at the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XiMb74ygcHs" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/03/11/WildFly23-Final-Released/</feedburner:origLink></entry><entry><title>Write your own Red Hat Ansible Tower inventory plugin</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ykaTXiaqjao/" /><category term="Automation" /><category term="Python" /><category term="Ansible" /><category term="Ansible inventories" /><category term="Ansible inventory plugin" /><category term="AWX" /><category term="Python 3" /><author><name>Rigel Di Scala</name></author><id>https://developers.redhat.com/blog/?p=780487</id><updated>2021-03-10T08:00:44Z</updated><published>2021-03-10T08:00:44Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/ansible/ansible"&gt;Ansible&lt;/a&gt; is an engine and language for automating many different IT tasks, such as provisioning a physical device, creating a virtual machine, or configuring an application and its dependencies. Ansible organizes these tasks in &lt;em&gt;playbook&lt;/em&gt; files, which run on one or more remote target hosts. &lt;em&gt;Inventory&lt;/em&gt; files maintain lists of these hosts and are formatted as YAML or INI documents. For example, a simple inventory file in INI format follows:&lt;/p&gt; &lt;pre&gt;[web] web1.example.com web2.example.com &lt;/pre&gt; &lt;p&gt;Ansible inventories can be &lt;em&gt;static&lt;/em&gt; (stored in a file and managed in a source code repository) or &lt;em&gt;dynamic&lt;/em&gt; (retrieved from an external web resource, such as through a RESTful API). Dynamic inventories are generated on-demand using &lt;em&gt;inventory scripts&lt;/em&gt; or &lt;em&gt;inventory plugins&lt;/em&gt;, consisting of code that Ansible runs to get a list of hosts to target when executing playbooks.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/reference_appendices/tower.html"&gt;Red Hat Ansible Tower&lt;/a&gt;, also known as &lt;a target="_blank" rel="nofollow" href="https://github.com/ansible/awx"&gt;AWX&lt;/a&gt; (the name of its upstream community project), is a front-end to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-ansible-engine/"&gt;Red Hat Ansible Engine&lt;/a&gt; that simplifies operations on large IT infrastructures. Operators can log into the Ansible Tower web interface and create single jobs or complex workflows using Ansible Engine building blocks such as tasks, roles, and playbooks. Enterprises typically manage assets in a configuration management database (CMDB), such as &lt;a target="_blank" rel="nofollow" href="https://netbox.readthedocs.io/en/stable/"&gt;NetBox&lt;/a&gt;, which Ansible Tower connects to using a specially written script or plugin.&lt;/p&gt; &lt;p&gt;This article shows you how to use Ansible Tower to create dynamic inventories. We&amp;#8217;ll start with a sample inventory script, then transform the script into a plugin. As you&amp;#8217;ll see, inventory plugins can accept parameters, which gives them an advantage over plain scripts.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Inventory scripts are &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/latest/html/administration/custom_inventory_script.html#"&gt;deprecated in Ansible Tower&lt;/a&gt;, so they will be removed in a future version. There’s a good reason: Source code is properly managed in a version control system, where developers and operators can track and review changes to its corpus.&lt;/p&gt; &lt;h2&gt;A sample inventory script&lt;/h2&gt; &lt;p&gt;Inventory scripts are organized in a single executable file, written in a scripting language such as Python or Bash. The script must return its data in JSON format. For instance, the following output provides the Ansible playbook with a list of hosts and related data:&lt;/p&gt; &lt;pre&gt;{ "all": { "hosts": ["web1.example.com", "web2.example.com"] }, "_meta": { "hostvars": { "web1.example.com": { "ansible_user": "root" }, "web2.example.com": { "ansible_user": "root" } } } } &lt;/pre&gt; &lt;p&gt;The following Bash code is an inventory script that generates the output just shown:&lt;/p&gt; &lt;pre&gt;#!/usr/bin/env bash # id: scripts/trivial-inventory-script.sh cat &amp;#60;&amp;#60; EOF { "all": { "hosts": ["web1.example.com", "web2.example.com"] }, "_meta": { "hostvars": { "web1.example.com": { "ansible_user": "rdiscala" }, "web2.example.com": { "ansible_user": "rdiscala" } } } } EOF &lt;/pre&gt; &lt;p&gt;Here, an Ansible command runs the inventory script and compares the actual output to the expected output:&lt;/p&gt; &lt;pre&gt;$ ansible -m ping -i scripts/trivial-inventory-script.sh all web1.example.com | SUCCESS =&amp;#62; { "ansible_facts": { "discovered_interpreter_python": "/usr/bin/python" }, "changed": false, "ping": "pong" } web2.example.com | SUCCESS =&amp;#62; { "ansible_facts": { "discovered_interpreter_python": "/usr/bin/python" }, "changed": false, "ping": "pong" } &lt;/pre&gt; &lt;p&gt;The output shows that Ansible correctly interpreted the information given in the &lt;code&gt;hostvars&lt;/code&gt; section and used my username &lt;code&gt;rdiscala&lt;/code&gt; to connect via SSH to the server hosts.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The sample script is intentionally brief and omits a detail: Ansible invokes these scripts with the &lt;code&gt;--list&lt;/code&gt; option if a list of hosts needs to be produced, as it does in our case. Alternatively, Ansible provides the &lt;code&gt;--host=NAME&lt;/code&gt; option when it needs the variables of a specific host, identified by its &lt;code&gt;NAME&lt;/code&gt;. To make the script fully compliant, you would need to implement logic to handle these options.&lt;/p&gt; &lt;h2&gt;Making scripts work in Ansible Tower&lt;/h2&gt; &lt;p&gt;Scripts are defined in the Inventory Scripts section of Ansible Tower&amp;#8217;s web interface. Alternatively, you can write a script in any scripting language supported on the Ansible Tower host. As shown in Figure 1, you can paste the script we&amp;#8217;ve just written directly into the &lt;b&gt;CUSTOM SCRIPT&lt;/b&gt; field and use it to sync an inventory inside Ansible Tower.&lt;/p&gt; &lt;div id="attachment_780867" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script.png"&gt;&lt;img aria-describedby="caption-attachment-780867" class="wp-image-780867" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-300x191.png" alt="Ansible Tower's Inventory Scripts screen contains a text field named CUSTOM SCRIPT, where an administrator can insert an inventory script." width="640" height="408" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-300x191.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-768x490.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script.png 936w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780867" class="wp-caption-text"&gt;Figure 1: You can plug a pre-written script into Ansible Tower&amp;#8217;s Inventory Scripts section.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can now use this new script as an &lt;em&gt;inventory source&lt;/em&gt; in any Ansible Tower inventory. An inventory source provides information about hosts to Ansible Tower on demand. When the source syncs, the script will run, fetch the data, and format it as shown previously so that Ansible Tower can import it into its own host database. The complete list of hosts will show up in the &lt;b&gt;HOSTS&lt;/b&gt; table, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_780877" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts.png"&gt;&lt;img aria-describedby="caption-attachment-780877" class="wp-image-780877 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts-1024x482.png" alt="Ansible Tower's Inventory Scripts screen contains a text field named CUSTOM SCRIPT, where an administrator can insert an inventory script." width="640" height="301" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts-1024x482.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts-768x362.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/tower-inventory-script-hosts.png 1036w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780877" class="wp-caption-text"&gt;Figure 2: Find the complete list of hosts in the HOSTS table.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Create an inventory plugin with Ansible Galaxy&lt;/h2&gt; &lt;p&gt;The newer and recommended way to distribute and consume Ansible content is to create an inventory plugin and package it as an &lt;a target="_blank" rel="nofollow" href="https://www.ansible.com/blog/getting-started-with-ansible-collections"&gt;Ansible collection&lt;/a&gt;. An inventory plugin is considered a module when packaged in a collection.&lt;/p&gt; &lt;p&gt;You can kickstart your effort by using the &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/cli/ansible-galaxy.html"&gt;Ansible Galaxy command-line program&lt;/a&gt; to create the basic structure for a collection:&lt;/p&gt; &lt;pre&gt;$ ansible-galaxy collection init zedr.blog_examples - Collection zedr.blog_examples was created successfully $ tree . . └── zedr └── blog_examples ├── docs ├── galaxy.yml ├── plugins │ └── README.md ├── README.md └── roles &lt;/pre&gt; &lt;p&gt;Let’s start with &lt;code&gt;galaxy.yml&lt;/code&gt;, the manifest file describes this collection:&lt;/p&gt; &lt;pre&gt;namespace: zedr name: blog_examples version: 1.0.0 readme: README.md authors: - Rigel Di Scala &amp;#60;rigel@redhat.com&amp;#62; &lt;/pre&gt; &lt;p&gt;We will create our plugin as a Python script named &lt;code&gt;example_hosts.py&lt;/code&gt; inside the &lt;code&gt;plugins/inventory&lt;/code&gt; folder. Placing the script in this location lets Ansible detect it as an inventory plugin. We can delete the &lt;code&gt;docs&lt;/code&gt; and &lt;code&gt;roles&lt;/code&gt; folders to focus on the minimum viable set of files needed to implement our collection. We should end up with a folder structure like this one:&lt;/p&gt; &lt;pre&gt;$ tree . . └── zedr └── blog_examples ├── galaxy.yml ├── plugins │ └── inventory │ └── example_hosts.py └── README.md &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Important&lt;/strong&gt;: Always specify the full namespace of the collection (for instance, &lt;code&gt;zedr.blog_examples&lt;/code&gt;) when referring to assets contained within it, such as roles and plugins.&lt;/p&gt; &lt;p&gt;We can now copy over, clean up, and populate the &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html"&gt;basic boilerplate code&lt;/a&gt; for an inventory plugin:&lt;/p&gt; &lt;pre&gt;from ansible.plugins.inventory import BaseInventoryPlugin ANSIBLE_METADATA = { 'metadata_version': '', 'status': [], 'supported_by': '' } DOCUMENTATION = ''' --- module: plugin_type: short_description: version_added: "" description: options: author: ''' class InventoryModule(BaseInventoryPlugin): """An example inventory plugin.""" NAME = 'FQDN_OF_THE_PLUGIN_GOES_HERE' def verify_file(self, path): """Verify that the source file can be processed correctly. Parameters: path:AnyStr The path to the file that needs to be verified Returns: bool True if the file is valid, else False """ def parse(self, inventory, loader, path, cache=True): """Parse and populate the inventory with data about hosts. Parameters: inventory The inventory to populate """ # The following invocation supports Python 2 in case we are # still relying on it. Use the more convenient, pure Python 3 syntax # if you don't need it. super(InventoryModule, self).parse(inventory, loader, path, cache) &lt;/pre&gt; &lt;h3&gt;About the code&lt;/h3&gt; &lt;p&gt;You&amp;#8217;ll note that this boilerplate defines two methods: &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html#verify-file"&gt;&lt;code&gt;verify_file()&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;parse()&lt;/code&gt;. Use &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html#verify-file"&gt;&lt;code&gt;verify_file()&lt;/code&gt;&lt;/a&gt; when the host list you want to process comes from a file, such as a CSV document, on a filesystem at a given path. This method is used to validate the file quickly before passing it to the more expensive &lt;code&gt;parse()&lt;/code&gt; method. Normally, &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html#verify-file"&gt;&lt;code&gt;verify_file()&lt;/code&gt;&lt;/a&gt; ensures that the file is valid incoming JSON and matches a predefined schema. (Note that the &lt;code&gt;verify_file()&lt;/code&gt; method is currently empty and must be filled in.)&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;verify_file()&lt;/code&gt; method can return &lt;code&gt;True&lt;/code&gt; when input comes from a source other than a file, such as when calling a remote HTTP API. But it could also verify the incoming JSON.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html#parse"&gt;&lt;code&gt;parse()&lt;/code&gt;&lt;/a&gt; method does most of the work of processing the source data to filter and format it correctly. However, instead of directly constructing the payload&amp;#8217;s &lt;code&gt;dict&lt;/code&gt; namespace, as we did in the inventory script, we will rely on the &lt;em&gt;instance attribute&lt;/em&gt;, &lt;code&gt;self.inventory&lt;/code&gt;, which is a special object with its own methods. The attribute offers &lt;code&gt;add_host()&lt;/code&gt; and &lt;code&gt;set_variable()&lt;/code&gt; methods to construct a data object suitable for Ansible to consume. (The &lt;code&gt;parse()&lt;/code&gt; method is currently empty except for a call to the superclass&amp;#8217;s function.)&lt;/p&gt; &lt;p&gt;Additionally, note that the module-level attributes &lt;code&gt;ANSIBLE_METADATA&lt;/code&gt; and &lt;code&gt;DOCUMENTATION&lt;/code&gt; are required, and that the &lt;code&gt;NAME&lt;/code&gt; attribute must have the plugin&amp;#8217;s fully qualified domain name, including the namespace.&lt;/p&gt; &lt;h3&gt;Invoking the plugin&lt;/h3&gt; &lt;p&gt;When the plugin is invoked in Ansible from the command line, the following chain of events occurs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The conventional name &lt;code&gt;InventoryModule&lt;/code&gt; is imported from the chosen inventory module (&lt;code&gt;zedr.blog_example.example_hosts.py&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;An instance of &lt;code&gt;InventoryModule&lt;/code&gt; is created.&lt;/li&gt; &lt;li&gt;The instance method &lt;code&gt;InventoryModule.verify_file()&lt;/code&gt; is called to perform an initial validation of the file (when applicable) and is expected to return a truthy value to proceed.&lt;/li&gt; &lt;li&gt;The instance method &lt;code&gt;InventoryModule.parse()&lt;/code&gt; is called to populate the &lt;code&gt;InventoryModule.inventory&lt;/code&gt; object.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;InventoryModule.inventory&lt;/code&gt; object is introspected to retrieve the host data that Ansible will consume.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We can now rewrite the script logic as follows:&lt;/p&gt; &lt;pre&gt;from ansible.plugins.inventory import BaseInventoryPlugin ANSIBLE_METADATA = { 'metadata_version': '1.0.0', 'status': ['preview'], 'supported_by': 'community' } DOCUMENTATION = ''' --- module: example_hosts plugin_type: inventory short_description: An example Ansible Inventory Plugin version_added: "2.9.13" description: - "A very simple Inventory Plugin created for demonstration purposes only." options: author: - Rigel Di Scala ''' class InventoryModule(BaseInventoryPlugin): """An example inventory plugin.""" NAME = 'zedr.blog_examples.example_hosts' def verify_file(self, path): """Verify that the source file can be processed correctly. Parameters: path:AnyStr The path to the file that needs to be verified Returns: bool True if the file is valid, else False """ # Unused, always return True return True def _get_raw_host_data(self): """Get the raw static data for the inventory hosts Returns: dict The host data formatted as expected for an Inventory Script """ return { "all": { "hosts": ["web1.example.com", "web2.example.com"] }, "_meta": { "hostvars": { "web1.example.com": { "ansible_user": "rdiscala" }, "web2.example.com": { "ansible_user": "rdiscala" } } } } def parse(self, inventory, *args, **kwargs): """Parse and populate the inventory with data about hosts. Parameters: inventory The inventory to populate We ignore the other parameters in the future signature, as we will not use them. Returns: None """ # The following invocation supports Python 2 in case we are # still relying on it. Use the more convenient, pure Python 3 syntax # if you don't need it. super(InventoryModule, self).parse(inventory, *args, **kwargs) raw_data = self._get_raw_host_data() _meta = raw_data.pop('_meta') for group_name, group_data in raw_data.items(): for host_name in group_data['hosts']: self.inventory.add_host(host_name) for var_key, var_val in _meta['hostvars'][host_name].items(): self.inventory.set_variable(host_name, var_key, var_val) &lt;/pre&gt; &lt;p&gt;Note that we have ignored facilities related to grouping and &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html#inventory-cache"&gt;caching&lt;/a&gt; to keep things simple. These facilities are worth looking into to organize the host list better and optimize the synchronization process&amp;#8217;s performance.&lt;/p&gt; &lt;h3&gt;Build, install, and test the plugin&lt;/h3&gt; &lt;p&gt;The next step is to build the Ansible collection package, install it locally, and test the plugin:&lt;/p&gt; &lt;pre&gt;$ cd zedr/blog_examples $ mkdir build $ ansible-galaxy collection build -f --output-path build Created collection for zedr.blog_examples at /home/rdiscala/blog/ansible-tower-inventory-plugin/collections/zedr/blog_examples/build/zedr-blog_examples-1.0.0.tar.gz $ ansible-galaxy collection install build/zedr-blog_examples-1.0.0.tar.gz Process install dependency map Starting collection install process Installing 'zedr.blog_examples:1.0.0' to '/home/rdiscala/.ansible/collections/ansible_collections/zedr/blog_examples' &lt;/pre&gt; &lt;p&gt;Next, we need to enable our plugin by adding a local &lt;code&gt;galaxy.cfg&lt;/code&gt; file in our current working directory. The contents are:&lt;/p&gt; &lt;pre&gt;[inventory] enable_plugins = zedr.blog_examples.example_hosts &lt;/pre&gt; &lt;p&gt;To check whether the local installation was successful, we can attempt to display the documentation for our inventory plugin, using its fully qualified domain name:&lt;/p&gt; &lt;pre&gt;$ ansible-doc -t inventory zedr.blog_examples.example_hosts &amp;#62; INVENTORY (/home/rdiscala/.ansible/collections/ansible_collections/zedr/blog_examples/plugins/inventory/example_hosts.py) An example Inventory Plugin created for demonstration purposes only. * This module is maintained by The Ansible Community AUTHOR: Rigel Di Scala &amp;#60;rigel@redhat.com&amp;#62; METADATA: status: - preview supported_by: community PLUGIN_TYPE: inventory &lt;/pre&gt; &lt;p&gt;We can also list the available plugins to verify that ours is detected correctly. Note that for this to work with the Ansible collection, you will need &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/ansible/#history2.10"&gt;Ansible version 3.0 or higher&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;$ ansible-doc -t inventory -l advanced_host_list Parses a 'host list' with ranges amazon.aws.aws_ec2 EC2 inventory source amazon.aws.aws_rds rds instance source auto Loads and executes an inventory plugin specified in a YAML config (...) zedr.blog_examples.example_hosts A trivial example of an Ansible Inventory Plugin &lt;/pre&gt; &lt;p&gt;Finally, we can test the plugin locally by running it using an inventory configuration file. Create a file named &lt;code&gt;inventory.yml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt;plugin: "zedr.blog_examples.example_hosts" &lt;/pre&gt; &lt;p&gt;Here is the command to invoke the plugin and generate the inventory data:&lt;/p&gt; &lt;pre&gt;$ ansible-inventory --list -i inventory.yml { "_meta": { "hostvars": { "web1.example.com": { "ansible_user": "rdiscala" }, "web2.example.com": { "ansible_user": "rdiscala" } } }, "all": { "children": [ "ungrouped" ] }, "ungrouped": { "hosts": [ "web1.example.com", "web2.example.com" ] } } &lt;/pre&gt; &lt;p&gt;Ansible has generated two &amp;#8220;virtual&amp;#8221; groups: &lt;code&gt;ungrouped&lt;/code&gt;, with our list of hosts, and &lt;code&gt;all&lt;/code&gt;, which includes &lt;code&gt;ungrouped&lt;/code&gt;. We have verified that the plugin is working correctly.&lt;/p&gt; &lt;h2&gt;Making the plugin work in Ansible Tower&lt;/h2&gt; &lt;p&gt;Ansible Tower can automate a collection&amp;#8217;s installation, making its roles and plugins available to projects and job templates. To make it work, we need the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A place to provide the package file that we built for our collection. We&amp;#8217;ll use a Git repo hosted on GitHub, but it could also be published on &lt;a target="_blank" rel="nofollow" href="https://galaxy.ansible.com/"&gt;Ansible Galaxy&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A repo for the project files containing the &lt;code&gt;requirements.yml&lt;/code&gt; file that references our collection and the &lt;code&gt;inventory.yml&lt;/code&gt; configuration file we used previously.&lt;/li&gt; &lt;li&gt;An Ansible Tower project that points to the project files repo.&lt;/li&gt; &lt;li&gt;An Ansible Tower inventory.&lt;/li&gt; &lt;li&gt;An Ansible Tower inventory source for our inventory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The following events will be triggered when Ansible Tower executes a job that uses this inventory:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The job triggers a project update (the internal &lt;code&gt;project_update.yml&lt;/code&gt; playbook).&lt;/li&gt; &lt;li&gt;The project syncs with its associated Git repo.&lt;/li&gt; &lt;li&gt;If necessary, the project installs any needed dependencies, which should be listed in the &lt;code&gt;collection/requirements.yml&lt;/code&gt; file.&lt;/li&gt; &lt;li&gt;The project update triggers an inventory update.&lt;/li&gt; &lt;li&gt;The inventory update triggers an inventory source sync.&lt;/li&gt; &lt;li&gt;The inventory source sync reads the inventory file &lt;code&gt;inventory.yml&lt;/code&gt; and runs our plugin to fetch the host data.&lt;/li&gt; &lt;li&gt;The host data populates the inventory.&lt;/li&gt; &lt;li&gt;The job runs the associated playbook on the inventory host list using the provided hostnames and variables.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Figure 3 shows this workflow.&lt;/p&gt; &lt;div id="attachment_780817" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-plugin-workflow.png"&gt;&lt;img aria-describedby="caption-attachment-780817" class="wp-image-780817 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-plugin-workflow-1024x512.png" alt="Visualizing the inventory-update process workflow just described." width="640" height="320" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-plugin-workflow-1024x512.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-plugin-workflow-300x150.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-plugin-workflow-768x384.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780817" class="wp-caption-text"&gt;Figure 3: The workflow for populating a host list using an inventory plugin.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, let&amp;#8217;s create the components required to make the plugin work.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The following example was tested on Ansible Tower 3.7.1.&lt;/p&gt; &lt;h3&gt;Create a Git repo for the collection&lt;/h3&gt; &lt;p&gt;To start, we&amp;#8217;ll create a new repo on Github and push the collection files we created earlier. A &lt;a target="_blank" rel="nofollow" href="https://github.com/zedr/blog_examples"&gt;sample repo&lt;/a&gt; is available on GitHub.&lt;/p&gt; &lt;p&gt;Ansible cannot clone a repository and build the collection by itself, so we need to build the package and make it available as a downloadable &lt;code&gt;tar.gz&lt;/code&gt; file. As an example, from the &lt;a href="https://github.com/zedr/blog_examples/releases/"&gt;Releases page&lt;/a&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: At the time of writing, Ansible Tower cannot fetch the package as an authenticated user, so you will need to allow anonymous clients.&lt;/p&gt; &lt;p&gt;If you are using GitHub, you can set up a GitHub Actions workflow to fully automate this process:&lt;/p&gt; &lt;pre&gt;# id: .github/workflows/main.yml name: CI # Only build releases when a new tag is pushed. on: push: tags: - '*' jobs: build: runs-on: ubuntu-latest steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 # Extract the version from the tag name so it can be used later. - name: Get the version id: get_version run: echo ::set-output name=VERSION::${GITHUB_REF#refs/tags/} # Install a recent version of Python 3 - name: Setup Python uses: actions/setup-python@v2 with: python-version: 3.7 # Install our dependencies, e.g. Ansible - name: Install Python 3.7 run: python3.7 -m pip install -r requirements.txt - name: Build the Ansible collection run: | mkdir -p build ansible-galaxy collection build -f --output-path build - name: Create a Release id: create_a_release uses: actions/create-release@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: tag_name: ${{ steps.get_version.outputs.VERSION }} release_name: Release ${{ steps.get_version.outputs.VERSION }} draft: false - name: Upload a Release Asset uses: actions/upload-release-asset@v1.0.2 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_a_release.outputs.upload_url }} asset_path: build/zedr-blog_examples-${{ steps.get_version.outputs.VERSION }}.tar.gz asset_name: "zedr-blog_examples-${{ steps.get_version.outputs.VERSION }}.tar.gz" asset_content_type: "application/gzip" &lt;/pre&gt; &lt;h3&gt;Create a Git repo for project files&lt;/h3&gt; &lt;p&gt;Next, we need another Git repo for the files that the Ansible Tower project will source. Here is the folder structure:&lt;/p&gt; &lt;pre&gt;$ tree . . ├── collections │ └── requirements.yml └── inventory.yml &lt;/pre&gt; &lt;p&gt;Note that &lt;code&gt;collections/requirements.yml&lt;/code&gt; will contain a reference to our Ansible collection package so that Ansible Tower can download, install, and use it when the inventory is synced. Additionally, the &lt;code&gt;inventory.yml&lt;/code&gt; is the same file we created earlier, containing the plugin&amp;#8217;s fully qualified domain name. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/zedr-automation/example_project"&gt;example repo&lt;/a&gt; for more details.&lt;/p&gt; &lt;h3&gt;Create a new Ansible Tower project&lt;/h3&gt; &lt;p&gt;Next, sign in to your Ansible Tower instance, create a new project, and fill in the following fields and checkboxes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Name&lt;/b&gt;: &lt;code&gt;My Project&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Organization&lt;/b&gt;: &lt;code&gt;Default&lt;/code&gt; (or whatever you prefer).&lt;/li&gt; &lt;li&gt;&lt;b&gt;SCM Type&lt;/b&gt;: &lt;code&gt;Git&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;SCM URL&lt;/b&gt;: &lt;code&gt;https://github.com/zedr-automation/example_project.git&lt;/code&gt; (or the Git repo URL of your project).&lt;/li&gt; &lt;li&gt;&lt;b&gt;SCM Branch/Tag/Commit&lt;/b&gt;: &lt;code&gt;master&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;SCM Update Options&lt;/b&gt;: select &lt;b&gt;Clean&lt;/b&gt;, &lt;b&gt;Delete On Update&lt;/b&gt;, and &lt;b&gt;Update Revision on Launch&lt;/b&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 4 shows the resulting form.&lt;/p&gt; &lt;div id="attachment_780857" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-project.png"&gt;&lt;img aria-describedby="caption-attachment-780857" class="wp-image-780857" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-project-300x246.png" alt="This form creates the Ansible Tower project." width="640" height="525" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-project-300x246.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-project-768x629.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-project.png 920w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780857" class="wp-caption-text"&gt;Figure 4: Creating the Ansible Tower project.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Create a new Ansible Tower inventory&lt;/h3&gt; &lt;p&gt;There are just two fields to create a new inventory in Tower: For the &lt;b&gt;Name&lt;/b&gt; field, enter &lt;code&gt;My Inventory&lt;/code&gt;. For the &lt;b&gt;Organization&lt;/b&gt;, you can select the default or whatever you previously entered. Figure 5 shows the resulting form.&lt;/p&gt; &lt;div id="attachment_780837" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-inventory.png"&gt;&lt;img aria-describedby="caption-attachment-780837" class="wp-image-780837" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-inventory-300x214.png" alt="This form creates the Ansible Tower inventory." width="640" height="456" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-inventory-300x214.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-inventory-768x548.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-inventory.png 923w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780837" class="wp-caption-text"&gt;Figure 5: Creating the Ansible Tower inventory.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Create a new inventory source for the inventory&lt;/h3&gt; &lt;p&gt;Finally, create a new inventory source for the inventory. Fill in the fields and checkboxes as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Name&lt;/b&gt;: &lt;code&gt;My inventory source&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Source&lt;/b&gt;: &lt;code&gt;Sourced from a project&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Project&lt;/b&gt;: &lt;code&gt;My project&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Inventory File&lt;/b&gt;: &lt;code&gt;inventory.yml&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Update Options&lt;/b&gt;: Select &lt;b&gt;Overwrite&lt;/b&gt;, &lt;b&gt;Overwrite Variables&lt;/b&gt;, and &lt;b&gt;Update on Project Update&lt;/b&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Save the form and then click the &lt;b&gt;Start sync process&lt;/b&gt; button for the new inventory source you just created. If the process finishes correctly, your inventory&amp;#8217;s HOSTS page will display the two example hosts, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_780827" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-hosts.png"&gt;&lt;img aria-describedby="caption-attachment-780827" class="wp-image-780827" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-hosts-300x148.png" alt="The two hosts just created appear in the hosts list in the Ansible Tower inventory." width="640" height="316" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-hosts-300x148.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-hosts-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/ansible-tower-hosts.png 999w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-780827" class="wp-caption-text"&gt;Figure 6: Viewing the HOSTS list in the Ansible Tower inventory.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Final thoughts&lt;/h2&gt; &lt;p&gt;The inventory plugin we&amp;#8217;ve created is basic, but it’s a good foundation for implementing more complex ones that can query external sources of data, perhaps using third-party libraries. Being modules, inventory plugins can also accept parameters, giving them an advantage over plain scripts. For more information, see the official Ansible documentation on &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible/latest/dev_guide/developing_plugins.html#plugin-configuration-documentation-standards"&gt;plugin configuration&lt;/a&gt;. Also, note that if you decide to use a third-party library not present in Python’s standard library, such as &lt;a target="_blank" rel="nofollow" href="https://requests.readthedocs.io/en/master/"&gt;Requests&lt;/a&gt;, you will need to install it manually in the appropriate &lt;a target="_blank" rel="nofollow" href="https://docs.ansible.com/ansible-tower/3.7.1/html/administration/tipsandtricks.html#using-virtualenv-with-at"&gt;Python virtual environment inside Ansible Tower&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy developing!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#38;linkname=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F10%2Fwrite-your-own-red-hat-ansible-tower-inventory-plugin%2F&amp;#038;title=Write%20your%20own%20Red%20Hat%20Ansible%20Tower%20inventory%20plugin" data-a2a-url="https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/" data-a2a-title="Write your own Red Hat Ansible Tower inventory plugin"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/"&gt;Write your own Red Hat Ansible Tower inventory plugin&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ykaTXiaqjao" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Ansible is an engine and language for automating many different IT tasks, such as provisioning a physical device, creating a virtual machine, or configuring an application and its dependencies. Ansible organizes these tasks in playbook files, which run on one or more remote target hosts. Inventory files maintain lists of these hosts and are formatted [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/"&gt;Write your own Red Hat Ansible Tower inventory plugin&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">780487</post-id><dc:creator>Rigel Di Scala</dc:creator><dc:date>2021-03-10T08:00:44Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/</feedburner:origLink></entry><entry><title type="html">ja.quarkus.io is now public</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FyAkPrXlrCU/" /><author><name /></author><id>https://quarkus.io/blog/ja-quarkus-io/</id><updated>2021-03-10T00:00:00Z</updated><content type="html">Today we’re proud to announce our quarkus.io Japanese localization site (https://ja.quarkus.io) is now open. It is where our valuable guides and blog entries are translated. Some contents have not been translated yet, but we are continuing to translate them one by one. We are working on the localization of the...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FyAkPrXlrCU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/ja-quarkus-io/</feedburner:origLink></entry><entry><title>An introduction to JavaScript SDK for CloudEvents</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QyzqZRtXZYY/" /><category term="Event-Driven" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="CloudEvents" /><category term="serverless functions" /><category term="Typescript" /><author><name>Lucas Holmquist</name></author><id>https://developers.redhat.com/blog/?p=810797</id><updated>2021-03-09T08:00:50Z</updated><published>2021-03-09T08:00:50Z</published><content type="html">&lt;p&gt;In today&amp;#8217;s world of &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; functions and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, events are everywhere. The problem is that they are described differently depending on the producer technology you use.&lt;/p&gt; &lt;p&gt;Without a common standard, the burden is on developers to constantly relearn how to consume events. Not having a standard also makes it more difficult for authors of libraries and tooling to deliver event data across environments like SDKs. Recently, a new project was created to help with this effort.&lt;/p&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; is a specification for describing event data in common formats to provide interoperability across services, platforms, and systems. In fact, Red Hat OpenShift Serverless Functions uses CloudEvents. For more information about this new developer feature, see &lt;a target="_blank" rel="nofollow" href="/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions/"&gt;&lt;em&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The CloudEvents specification&lt;/h2&gt; &lt;p&gt;The specification&amp;#8217;s goal isn’t to create yet another event format and try to force everyone to use it. Rather, we want to define common metadata for events and establish where this metadata should appear in the message being sent.&lt;/p&gt; &lt;p&gt;It is a simple spec with simple goals. In fact, a CloudEvent requires only four pieces of metadata:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;type&lt;/code&gt; describes what kind of event this might be (e.g., a “create” event).&lt;/li&gt; &lt;li&gt;&lt;code&gt;specversion&lt;/code&gt; denotes the version of the spec used to create the CloudEvent.&lt;/li&gt; &lt;li&gt;&lt;code&gt;source&lt;/code&gt; describes where the event came from.&lt;/li&gt; &lt;li&gt;&lt;code&gt;id&lt;/code&gt; is a unique identifier that is useful for de-duping.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There are other useful fields, like &lt;code&gt;subject&lt;/code&gt;, which when combined with &lt;code&gt;source&lt;/code&gt; can add a little more context to where the event originated.&lt;/p&gt; &lt;p&gt;As I mentioned, the CloudEvents specification is only concerned with the common metadata listed above, and the location where this metadata is placed when sending the event.&lt;/p&gt; &lt;p&gt;Currently, there are two event formats: Binary, which is the preferred format, and structured. Binary is recommended because it is additive. That is, the binary format only adds some headers to the HTTP request. If there is a middleware that doesn’t understand CloudEvents, it won’t break anything, but if that system is updated to support CloudEvents, it starts working.&lt;/p&gt; &lt;p&gt;Structured formats are for those who don’t have any format currently defined and are looking for guidance on how things should be structured.&lt;/p&gt; &lt;p&gt;Here is a quick example of what those two event formats might look like in raw HTTP:&lt;/p&gt; &lt;pre&gt;// Binary Post /event HTTP/1.0 Host: example.com Content-Type: application/json ce-specversion: 1.0 ce-type: com.nodeshift.create ce-source: nodeshift.dev ce-id: 123456 { "action": "createThing", "item": "2187" } // Structured Post /event HTTP/1.0 Host: example.com Content-Type: application/cloudevents+json { "specversion": "1.0" "type": "com.nodeshift.create" "source": "nodeshift.dev" "id": "123456" "data": { "action": "createThing", "item": "2187" } } &lt;/pre&gt; &lt;h2&gt;JavaScript SDK for CloudEvents&lt;/h2&gt; &lt;p&gt;Of course, we don’t want to have to format these events manually. That is where the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/cloudevents"&gt;JavaScript SDK for CloudEvents&lt;/a&gt; comes in. There are three main goals that an SDK should accomplish:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compose an event.&lt;/li&gt; &lt;li&gt;Encode an event for sending.&lt;/li&gt; &lt;li&gt;Decode an incoming event.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Installing the JavaScript SDK is like using any other Node module:&lt;/p&gt; &lt;pre&gt;$ npm install cloudevents &lt;/pre&gt; &lt;p&gt;Now that we’ve seen what a CloudEvent is and how it is useful let&amp;#8217;s take a look at an example.&lt;/p&gt; &lt;h2&gt;Create a new CloudEvent&lt;/h2&gt; &lt;p&gt;First, we are going to create a new CloudEvent object:&lt;/p&gt; &lt;pre&gt;const { CloudEvent } = require('cloudevents'); // Create a new CloudEvent const ce = new CloudEvent({ type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', data: { key: 'DATA' } }); &lt;/pre&gt; &lt;p&gt;If we log this out with the object&amp;#8217;s built-in &lt;code&gt;toJSON&lt;/code&gt; method, we might see something like this:&lt;/p&gt; &lt;pre&gt;console.log(ce.toJSON()); { id: '...', type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', specversion: '1.0', time: '...', data: { key: 'DATA' } } &lt;/pre&gt; &lt;h3&gt;Sending the message&lt;/h3&gt; &lt;p&gt;Next, let&amp;#8217;s look at how to send this over HTTP using the binary format.&lt;/p&gt; &lt;p&gt;First, we need to create our message in the binary format, which you can do easily with the &lt;code&gt;HTTP.binary&lt;/code&gt; method. We will use the CloudEvent from the previous example:&lt;/p&gt; &lt;pre&gt; const message = HTTP.binary(ce); //const message = HTTP.structured(ce); // Showing just for completeness &lt;/pre&gt; &lt;p&gt;Again, if we log this out, it might look something like this:&lt;/p&gt; &lt;pre&gt; headers: { 'content-type': 'application/json;', 'ce-id': '...', 'ce-type': 'com.cloudevent.fun', 'ce-source': 'fun-with-cloud-events', 'ce-specversion': '1.0', 'ce-time': '...' }, body: { key: 'DATA' } } &lt;/pre&gt; &lt;p&gt;Now that the message has been formatted properly, we can send it by using a library like &lt;a target="_blank" rel="nofollow" href="https://github.com/axios/axios"&gt;Axios&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note that the CloudEvents SDK doesn’t handle sending messages; it only handles formatting the message headers and message body. This allows you to use any HTTP library you want to send the message.&lt;/p&gt; &lt;pre&gt;const axios = require('axios') axios({ method: 'post', url: 'http://localhost:3000/cloudeventy', data: message.body, headers: message.headers }).then((response) =&amp;#62; { console.log(response.data); }); &lt;/pre&gt; &lt;p&gt;We are sending a POST request to the “cloudevent-y” REST endpoint. In this example, I have used a simple Express.js application, but you can use any framework you like.&lt;/p&gt; &lt;h3&gt;Receiving the message&lt;/h3&gt; &lt;p&gt;Once we have the message, we can use the &lt;code&gt;HTTP.toEvent&lt;/code&gt; method to convert it back into a CloudEvent object.&lt;/p&gt; &lt;pre&gt;const express = require('express'); const { HTTP } = require('cloudevents'); const app = express(); app.post('/cloudeventy', (req, res) =&amp;#62; { const ce = HTTP.toEvent({ headers: req.headers, body: req.body }); console.log(ce.toJSON()); res.send({key: 'Event Received'}); }); &lt;/pre&gt; &lt;p&gt;Again, the log output looks similar to what we saw when we output the CloudEvent object:&lt;/p&gt; &lt;pre&gt;{ id: '...', type: 'com.cloudevent.fun', source: 'fun-with-cloud-events', specversion: '1.0', time: '...', data: { key: 'DATA' } } &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;To learn more about the JavaScript SDK for CloudEvents, &lt;a target="_blank" rel="nofollow" href="https://github.com/cloudevents/sdk-javascript"&gt;check out the GitHub project&lt;/a&gt;. For more information about the history, development, and design rationale behind the specification, see the &lt;a href="https://github.com/cloudevents/spec/blob/master/primer.md" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;CloudEvents Primer&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#38;linkname=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fan-introduction-to-javascript-sdk-for-cloudevents%2F&amp;#038;title=An%20introduction%20to%20JavaScript%20SDK%20for%20CloudEvents" data-a2a-url="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/" data-a2a-title="An introduction to JavaScript SDK for CloudEvents"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/"&gt;An introduction to JavaScript SDK for CloudEvents&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QyzqZRtXZYY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In today&amp;#8217;s world of serverless functions and microservices, events are everywhere. The problem is that they are described differently depending on the producer technology you use. Without a common standard, the burden is on developers to constantly relearn how to consume events. Not having a standard also makes it more difficult for authors of libraries [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/"&gt;An introduction to JavaScript SDK for CloudEvents&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">810797</post-id><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-03-09T08:00:50Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/</feedburner:origLink></entry><entry><title>Deploying Node.js applications to Kubernetes with Nodeshift and Minikube</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fAqJZg6t3vY/" /><category term="Developer Tools" /><category term="JavaScript" /><category term="Kubernetes" /><category term="Node.js" /><category term="minikube" /><category term="nodeshift" /><category term="openshift" /><category term="S2I" /><author><name>Lucas Holmquist</name></author><id>https://developers.redhat.com/blog/?p=865157</id><updated>2021-03-09T08:00:39Z</updated><published>2021-03-09T08:00:39Z</published><content type="html">&lt;p&gt;In a &lt;a target="_blank" rel="nofollow" href="/blog/2019/08/30/easily-deploy-node-js-applications-to-red-hat-openshift-using-nodeshift/"&gt;previous article&lt;/a&gt;, I showed how easy it was to deploy a &lt;a target="_blank" rel="nofollow" href="/topics/nodejs"&gt;Node.js&lt;/a&gt; application during development to &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; using the Nodeshift command-line interface (CLI). In this article, we will take a look at using Nodeshift to deploy Node.js applications to vanilla &lt;a target="_blank" rel="nofollow" href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;—specifically, with Minikube.&lt;/p&gt; &lt;h2&gt;Getting started&lt;/h2&gt; &lt;p&gt;If you want to follow along with this tutorial, you will need to run Minikube. I won&amp;#8217;t cover the setup process, but &lt;a target="_blank" rel="nofollow" href="https://minikube.sigs.k8s.io/docs/start/"&gt;Minikube&amp;#8217;s documentation&lt;/a&gt; can guide you through it. For the tutorial, I also assume that you have installed &lt;a target="_blank" rel="nofollow" href="https://nodejs.org/en/download"&gt;Node.js and Node Package Manager (npm)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The code samples we&amp;#8217;ll use are available on &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift-starters/basic-node-app-dockerized"&gt;GitHub&lt;/a&gt;. Our example is a very basic Node.js application with a Dockerfile. In fact, it is taken from the &lt;a target="_blank" rel="nofollow" href="https://nodejs.org/en/docs/guides/nodejs-docker-webapp/"&gt;&lt;i&gt;Dockerizing a Node.js web app&lt;/i&gt;&lt;/a&gt; guide on Nodejs.org.&lt;/p&gt; &lt;h2&gt;The Nodeshift CLI&lt;/h2&gt; &lt;p&gt;As the Nodeshift module readme states, Nodeshift is an opinionated command-line application and programmable API that you can use to deploy Node.js applications to &lt;a target="_blank" rel="nofollow" href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. You can easily run it using the &lt;code&gt;npx&lt;/code&gt; command, and it will create the appropriate YAML files to deploy your application.&lt;/p&gt; &lt;p&gt;Nodeshift is a great tool to use if you are developing against an OpenShift cluster, which uses the Source-to-Image (S2I) workflow. In short, Nodeshift creates an OpenShift &lt;code&gt;BuildConfig&lt;/code&gt;, which calls a Node.js S2I image to build your Node application. In most cases, you can achieve this by running &lt;code&gt;npm install&lt;/code&gt;. The build result is put into an OpenShift &lt;code&gt;ImageStream&lt;/code&gt; that resides in the internal OpenShift container registry. This image is then used to deploy your application.&lt;/p&gt; &lt;p&gt;But what about deploying to a vanilla Kubernetes cluster that doesn’t know anything about BuildConfigs, ImageStreams, or S2I? Well, as of &lt;a href="https://github.com/nodeshift/nodeshift/releases/tag/v7.3.0"&gt;Nodeshift&amp;#8217;s 7.3 release&lt;/a&gt;, you can now deploy your Node.js applications to Minikube.&lt;/p&gt; &lt;h2&gt;Deploying Node.js to Minikube&lt;/h2&gt; &lt;p&gt;Before we look at how Nodeshift works for deploying a Node.js application to Minikube, let’s take a minute for a high-level overview of deploying to Kubernetes.&lt;/p&gt; &lt;p&gt;First, you will create an application container image, which you can do with Docker. Once you have a container image, you&amp;#8217;ll need to push that image to a container registry that your cluster has access to, something like &lt;a target="_blank" rel="nofollow" href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt;. Once the image is available, you must then specify that image in your deployment YAML and create a service to expose the application.&lt;/p&gt; &lt;p&gt;This flow starts to be more cumbersome when you start iterating on your code. It isn’t really development-friendly if you need to run a Docker build and push that new image to Docker Hub every time. Not to mention that you also need to update your deployment with the new version of the image to ensure it redeploys.&lt;/p&gt; &lt;p&gt;Nodeshift&amp;#8217;s goal is to make developers&amp;#8217; lives easier when deploying to OpenShift and Kubernetes. Let&amp;#8217;s see how Nodeshift helps with each of those unwieldy steps.&lt;/p&gt; &lt;h2&gt;Minikube&amp;#8217;s internal Docker server&lt;/h2&gt; &lt;p&gt;A major difference between OpenShift and Kubernetes is that there is no easy way to run S2I builds on plain Kubernetes. We also don’t want to run a Docker build and push to Docker Hub every time we change our code. Fortunately, Minikube gives us an alternative.&lt;/p&gt; &lt;p&gt;Minikube has its own internal Docker server that we can connect to using the &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/engine/api/v1.41/#"&gt;Docker Engine API&lt;/a&gt;. We can use this server to run our Docker build in the environment, which means that we don’t have to push the image to an external resource like Docker Hub. We can then use this image in our deployment.&lt;/p&gt; &lt;p&gt;To get access to the internal Docker server, Minikube has a command to export some environment variables to add to your terminal shell. This command is &lt;code&gt;minikube docker-env&lt;/code&gt;, which might output something like this:&lt;/p&gt; &lt;pre&gt;export DOCKER_TLS_VERIFY="1" export DOCKER_HOST="tcp://192.168.39.12:2376" export DOCKER_CERT_PATH="/home/lucasholmquist/.minikube/certs" export MINIKUBE_ACTIVE_DOCKERD="minikube" # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) &lt;/pre&gt; &lt;h2&gt;Making it easier with Nodeshift&lt;/h2&gt; &lt;p&gt;Nodeshift abstracts the details we don’t really care about so we can focus on our applications. In this case, we don’t want to think about how to connect to Minikube&amp;#8217;s internal server or how to run Docker commands by hand, and we don’t want to think about updating our deployment YAML every time we build a new image to redeploy it.&lt;/p&gt; &lt;p&gt;Using the Nodeshift CLI with the &lt;code&gt;--kube&lt;/code&gt; flag simplifies those tasks. Let’s see how it works using our &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift-starters/basic-node-app-dockerized"&gt;example application&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We will use &lt;code&gt;npx&lt;/code&gt; to deploy the Node.js application to Minikube, so we don’t need to install anything globally. Run it like this in the example directory:&lt;/p&gt; &lt;pre&gt;$ npx nodeshift --kube &lt;/pre&gt; &lt;p&gt;Nodeshift creates a service and deployment by default if none are provided. Also, note that the type of service it creates is a &lt;code&gt;LoadBalancer&lt;/code&gt;, which allows us to expose our application without using ingress.&lt;/p&gt; &lt;p&gt;The Nodeshift CLI runs the same &lt;code&gt;goals&lt;/code&gt; for a Kubernetes deploy as it does for an OpenShift deploy. The key difference comes during the &lt;code&gt;build&lt;/code&gt; phase. Instead of creating an OpenShift &lt;code&gt;BuildConfig&lt;/code&gt; and running an S2I process on the cluster, Nodeshift uses the &lt;a target="_blank" rel="nofollow" href="https://www.npmjs.com/package/dockerode"&gt;dockerode&lt;/a&gt; module to connect to Minikube&amp;#8217;s internal Docker server and run a build using the provided Dockerfile. The built image is now in that internal registry, ready to be deployed by the deployment YAML that the Nodeshift CLI creates. Nodeshift also adds a randomly-generated number to the deployment&amp;#8217;s metadata, which is then applied during every redeploy. This will trigger Minikube to redeploy the application with the new image.&lt;/p&gt; &lt;p&gt;The following is an example log output:&lt;/p&gt; &lt;pre&gt;~/develop/nodeshift-starters/basic-node-app-dockerized» npx nodeshift --kube 2021-02-09T20:03:18.405Z INFO loading configuration 2021-02-09T20:03:18.452Z INFO Using the kubernetes flag. 2021-02-09T20:03:18.762Z INFO using namespace default at https://192.168.39.12:8443 2021-02-09T20:03:18.763Z WARNING a file property was not found in your package.json, archiving the current directory. 2021-02-09T20:03:18.773Z INFO creating archive of .dockerignore, .gitignore, Dockerfile, README.md, package-lock.json, package.json, server.js 2021-02-09T20:03:18.774Z INFO Building Docker Image 2021-02-09T20:03:18.848Z TRACE {"stream":"Step 1/7 : FROM node:14"} 2021-02-09T20:03:18.848Z TRACE {"stream":"\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e cb544c4472e9\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":"Step 2/7 : WORKDIR /usr/src/app"} 2021-02-09T20:03:18.849Z TRACE {"stream":"\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e Using cache\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":" ---\u003e 57c9e3a4e918\n"} 2021-02-09T20:03:18.849Z TRACE {"stream":"Step 3/7 : COPY package*.json ./"} 2021-02-09T20:03:18.850Z TRACE {"stream":"\n"} 2021-02-09T20:03:19.050Z TRACE {"stream":" ---\u003e 742050ca3266\n"} 2021-02-09T20:03:19.050Z TRACE {"stream":"Step 4/7 : RUN npm install"} 2021-02-09T20:03:19.050Z TRACE {"stream":"\n"} 2021-02-09T20:03:19.109Z TRACE {"stream":" ---\u003e Running in f3477d5f2b00\n"} 2021-02-09T20:03:21.739Z TRACE {"stream":"\u001b[91mnpm WARN basic-node-app-dockerized@1.0.0 No description\n\u001b[0m"} 2021-02-09T20:03:21.744Z TRACE {"stream":"\u001b[91mnpm WARN basic-node-app-dockerized@1.0.0 No repository field.\n\u001b[0m"} 2021-02-09T20:03:21.745Z TRACE {"stream":"\u001b[91m\n\u001b[0m"} 2021-02-09T20:03:21.746Z TRACE {"stream":"added 50 packages from 37 contributors and audited 50 packages in 1.387s\n"} 2021-02-09T20:03:21.780Z TRACE {"stream":"found 0 vulnerabilities\n\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":"Removing intermediate container f3477d5f2b00\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":" ---\u003e afb97a82c035\n"} 2021-02-09T20:03:22.303Z TRACE {"stream":"Step 5/7 : COPY . ."} 2021-02-09T20:03:22.303Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.481Z TRACE {"stream":" ---\u003e 1a451003c472\n"} 2021-02-09T20:03:22.481Z TRACE {"stream":"Step 6/7 : EXPOSE 8080"} 2021-02-09T20:03:22.482Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.545Z TRACE {"stream":" ---\u003e Running in a76389d44b59\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":"Removing intermediate container a76389d44b59\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":" ---\u003e 8ee240b7f9ab\n"} 2021-02-09T20:03:22.697Z TRACE {"stream":"Step 7/7 : CMD [ \"node\", \"server.js\" ]"} 2021-02-09T20:03:22.698Z TRACE {"stream":"\n"} 2021-02-09T20:03:22.759Z TRACE {"stream":" ---\u003e Running in 1f7325ab3c64\n"} 2021-02-09T20:03:22.911Z TRACE {"stream":"Removing intermediate container 1f7325ab3c64\n"} 2021-02-09T20:03:22.912Z TRACE {"stream":" ---\u003e d7f5d1e95592\n"} 2021-02-09T20:03:22.912Z TRACE {"aux":{"ID":"sha256:d7f5d1e9559242f767b54b168c36df5c7cbce6ebc7eb1145d7f6292f20e8cda2"}} 2021-02-09T20:03:22.913Z TRACE {"stream":"Successfully built d7f5d1e95592\n"} 2021-02-09T20:03:22.929Z TRACE {"stream":"Successfully tagged basic-node-app-dockerized:latest\n"} 2021-02-09T20:03:22.933Z WARNING No .nodeshift directory 2021-02-09T20:03:22.954Z INFO openshift.yaml and openshift.json written to /home/lucasholmquist/develop/nodeshift-starters/basic-node-app-dockerized/tmp/nodeshift/resource/ 2021-02-09T20:03:22.975Z INFO creating new service basic-node-app-dockerized 2021-02-09T20:03:22.979Z TRACE Deployment Applied 2021-02-09T20:03:23.036Z INFO Application running at: http://192.168.39.12:30076 2021-02-09T20:03:23.036Z INFO complete &lt;/pre&gt; &lt;p&gt;Following the deployment, the Nodeshift CLI also provides the URL where the application is running in the console output. The output might look something like this:&lt;/p&gt; &lt;pre&gt;... INFO Application running at http://192.168.39.12:30769 ... &lt;/pre&gt; &lt;p&gt;Navigating to the URL provided returns &amp;#8220;Hello World.&amp;#8221;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article gave a brief overview of the Nodeshift CLI&amp;#8217;s support for deploying to Minikube. In the future, we plan to add more Kubernetes platforms and other developer-friendly features, like possibly having the Nodeshift CLI create a default Dockerfile if there isn’t one.&lt;/p&gt; &lt;p&gt;If you like what you see and want to learn more, check out the &lt;a target="_blank" rel="nofollow" href="https://nodeshift.dev/"&gt;Nodeshift project&lt;/a&gt;. As always, if there are more features you would like to see, &lt;a target="_blank" rel="nofollow" href="https://github.com/nodeshift/nodeshift"&gt;create an issue&lt;/a&gt; over on GitHub. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js landing page.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#38;linkname=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F03%2F09%2Fdeploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube%2F&amp;#038;title=Deploying%20Node.js%20applications%20to%20Kubernetes%20with%20Nodeshift%20and%20Minikube" data-a2a-url="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/" data-a2a-title="Deploying Node.js applications to Kubernetes with Nodeshift and Minikube"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/"&gt;Deploying Node.js applications to Kubernetes with Nodeshift and Minikube&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fAqJZg6t3vY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In a previous article, I showed how easy it was to deploy a Node.js application during development to Red Hat OpenShift using the Nodeshift command-line interface (CLI). In this article, we will take a look at using Nodeshift to deploy Node.js applications to vanilla Kubernetes—specifically, with Minikube. Getting started If you want to follow along [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/"&gt;Deploying Node.js applications to Kubernetes with Nodeshift and Minikube&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">865157</post-id><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-03-09T08:00:39Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/</feedburner:origLink></entry></feed>
